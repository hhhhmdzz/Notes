# 由前向传播和反向传播联想到

## 矩阵乘法的时间复杂度和计算量

假设有两个矩阵进行矩阵乘法 $C=AB$，矩阵 $A,B,C$ 的形状分别为 $(N, M), (M, P), (N, P)$。矩阵 $C$ 有 $NP$ 个元素，每个元素需要进行 $M$ 次乘法和 $M-1$ 次加法得到，所以矩阵乘法的**时间复杂度**为 $O(NMP)$，**计算量**为 $O\left( \left(2 M - 1 \right) N P \right) \approx O(2NMP)$。多个矩阵进行矩阵乘法的时间复杂度和计算量同理，略。

## 矩阵求导的链式法则

假设有向量 $\vec{x} = \left(x_1, x_2, \dots, x_n \right), \vec{u} = \left(u_1, u_2, \dots, u_k \right), \vec{y} = \left(y_1, y_2, \dots, y_m \right)$，有向量求导规则如下：

1. $y = f(x), \frac{d y}{d x} = f^{'}$
2. $y = f(\vec{x}), \frac{\partial y}{\partial \vec{x}} = \left(\frac{\partial  y}{\partial x_1}, \frac{\partial  y}{\partial x_2}, \dots, \frac{\partial  y}{\partial x_n}\right)$
3. $\vec{y} = f(\vec{x}), \frac{\partial \vec{y}}{\partial \vec{x}} = \left[\begin{matrix} \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \dots & \frac{\partial y_1}{\partial x_n} \\ \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \dots & \frac{\partial y_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \dots & \frac{\partial y_m}{\partial x_n} \\ \end{matrix} \right]$（雅可比矩阵）

假设有 $\bold{\hat{Y}} = \bold{A} \bold{X} + \bold{B}, L = F(\bold{\hat{Y}}, \bold{Y}) = \sum_{i_p=1}^{P} \sum_{i_m=1}^{M} f(\hat{y}_{i_m,i_p}, y_{i_m,i_p})$，其中矩阵 $\bold{\hat{Y}}, \bold{A}, \bold{X}, \bold{B}$ 的形状分别为 $(M, P), (M, N), (N, P), (M, P)$，矩阵元素使用小写字母表示为 $\hat{y}, a, x, b$。

其中 $\hat{y}_{i_m,i_p} = \sum_{i_n=1}^{N} a_{i_m,i_n} x_{i_n,i_p}$，则有 $\frac{\partial \hat{y}_{i_m,i_p}}{\partial x_{n,i_p}} = a_{i_m,n}$

求 $L$ 关于 $x_{n,p}$ 的偏导：
$$
\begin{array}{l l}
\frac{\partial L}{\partial x_{n,p}} &= \frac{\partial }{\partial x_{n,p}} \sum_{i_p=1}^{P} \sum_{i_m=1}^{M} f(\hat{y}_{i_m,i_p}, y_{i_m,i_p}) \\
&= \frac{\partial }{\partial x_{n,p}} \sum_{i_m=1}^{M} f(\hat{y}_{i_m,p}, y_{i_m,p}) \\
&= \sum_{i_m=1}^{M} \frac{\partial }{\partial \hat{y}_{i_m,p} } \frac{\partial \hat{y}_{i_m,p} }{\partial x_{n,p}} f(\hat{y}_{i_m,p}, y_{i_m,p}) \\
&= \sum_{i_m=1}^{M} \frac{\partial f(\hat{y}_{i_m,p}, y_{i_m,p})}{\partial \hat{y}_{i_m,p} } \frac{\partial \hat{y}_{i_m,p} }{\partial x_{n,p}} \\
&= \sum_{i_m=1}^{M} a_{i_m,n} \frac{\partial f(\hat{y}_{i_m,p}, y_{i_m,p})}{\partial \hat{y}_{i_m,p} } \\
&= \bold{A}_{:,n} \frac{\partial L}{\partial \bold{\hat{Y}}_{:,p}} \\
\end{array}
$$
其中，$\bold{A}_{:,n}$ 表示矩阵 $\bold{A}$ 的第 $n$ 列。

所以矩阵求导的**链式法则**为：
$$
\frac{\partial{L}}{\partial{\bold{X}}} = \frac{\partial{L}}{\partial{\bold{\hat{Y}}}} \frac{\partial{\bold{\hat{Y}}}}{\partial{\bold{X}}} = \frac{\partial{\bold{\hat{Y}}}}{\partial{\bold{X}}} \cdot \frac{\partial{L}}{\partial{\bold{\hat{Y}}}}= \bold{A}^T \cdot \frac{\partial{L}}{\partial{\bold{\hat{Y}}}}
$$

当然这里也可以使用 $\frac{\partial{\left( \bold{A} \bold{X} + \bold{B} \right)}}{\partial{\bold{X}}} = \bold{A}^T$ 推导得到。

> 注：这里为了方便理解，使用 $\frac{\partial{L}}{\partial{\bold{\hat{Y}}}} \frac{\partial{\bold{\hat{Y}}}}{\partial{\bold{X}}}$ 直观地展示矩阵求导的链式关系，但是这并不代表实际的矩阵求导的矩阵乘法运算，$\frac{\partial{\bold{\hat{Y}}}}{\partial{\bold{X}}} \cdot \frac{\partial{L}}{\partial{\bold{\hat{Y}}}}$ 才是正确的运算。

## 前向传播和反向传播之间的关系

假设有以下MLP模型，且激活函数为 $\text{act}(x) = x$，损失函数为均方误差损失。

<p align="center">
    <img src="assets/mlp.svg" width="400"/>
<p>

不妨假设 $w_1, w_2, w_3$ 的维度分别为 $(M_0, M_1), (M_1, M_2), (M_2, M_3)$

那么 $x, z_1, z_2, \hat{y}$ 的维度分别为 $(B, M_0), (B, M_1), (B, M_2), (B, M_3)$

- 前向传播

前向传播过程为：
$$
\begin{array}{r l}
z_1 &= \text{act}(h_1), h_1 = x w_1 \\
z_2 &= \text{act}(h_2), h_2 = z_1 w_2 \\
\hat{y} &= \text{act}(h_3), h_3 = z_2 w_3 \\
L &= L(x, y, w_1, w_2, w_3) = \Vert\hat{y} - y\Vert_2^2 \\
\end{array}
$$

前向传播计算量主要为矩阵乘法和激活函数：

> 注：激活函数是逐元素矩阵乘法

第一层计算量为 $O(2BM_0M_1 + BM_1) \approx O(2BM_0M_1)$

第二层计算量为 $O(2BM_1M_2 + BM_2) \approx O(2BM_1M_2)$

第三层计算量为 $O(2BM_2M_3 + BM_3) \approx O(2BM_2M_3)$

损失函数计算量为 $O(2BM_3)$

可以看出，前向传播计算量主要为矩阵乘法。将MLP模型扩展至 $N+1$ 层，且每个隐藏层的维度为 $M$，最后输出的维度为 $C$，可以得到该MLP模型前向传播的计算量为 $O(2NBM^2+2BMC) \approx O(2BNM^2)$。

- 反向传播

反向传播过程为：
$$
\begin{array}{l l}
w_3 &:= w_3 - \alpha \cdot \frac{\partial L}{\partial w_3}
&:= w_3 - \alpha \cdot {\color{blue} {\color{red} \frac{\partial L}{\partial \hat{y}}} {\color{green} \frac{\partial \hat{y}}{\partial h_3}} \frac{\partial h_3}{\partial w_3}} \\
w_2 &:= w_2 - \alpha \cdot \frac{\partial L}{\partial w_2}
&:= w_2 - \alpha \cdot \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial h_3} {\color{red} \frac{\partial h_3}{\partial z_2}} {\color{green} \frac{\partial z_2}{\partial h_2}} {\color{blue} \frac{\partial h_2}{\partial w_2}} \\
w_1 &:= w_1 - \alpha \cdot \frac{\partial L}{\partial w_1}
&:= w_1 - \alpha \cdot \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial h_3} \frac{\partial h_3}{\partial z_2} \frac{\partial z_2}{\partial h_2} {\color{red} \frac{\partial h_2}{\partial z_1}} {\color{green} \frac{\partial z_1}{\partial h_1}} {\color{blue} \frac{\partial h_1}{\partial w_1}} \\


\end{array}
$$
即：
$$
\begin{array}{l l}

w_3 &:= w_3 - \alpha \cdot {\color{blue} \frac{\partial h_3}{\partial w_3}} \cdot {\color{red} \frac{\partial L}{\partial \hat{y}}} \odot {\color{green} \frac{\partial \hat{y}}{\partial h_3}}
& = w_3 - \alpha \cdot {\color{blue} z_2^T} \cdot {\color{red} 2 \left(\hat{y} - y\right)} \odot {\color{green} 1} \\

w_2 &:= w_2 - \alpha \cdot {\color{blue} \frac{\partial h_2}{\partial w_2}} \cdot \frac{\partial L}{\partial \hat{y}} \odot \frac{\partial \hat{y}}{\partial h_3} \cdot {\color{red} \frac{\partial h_3}{\partial z_2}} \odot {\color{green} \frac{\partial z_2}{\partial h_2}}
& = w_2 - \alpha \cdot {\color{blue} z_1^T} \cdot 2 \left(\hat{y} - y\right)^T \odot 1 \cdot {\color{red} w_3^T} \odot {\color{green} 1} \\

w_1 &:= w_1 - \alpha \cdot {\color{blue} \frac{\partial h_1}{\partial w_1}} \cdot \frac{\partial L}{\partial \hat{y}} \odot \frac{\partial \hat{y}}{\partial h_3} \cdot \frac{\partial h_3}{\partial z_2} \odot \frac{\partial z_2}{\partial h_2} \cdot {\color{red} \frac{\partial h_2}{\partial z_1}} \odot {\color{green} \frac{\partial z_1}{\partial h_1}}
& = w_1 - \alpha  \cdot {\color{blue} x^T} \cdot 2 \left(\hat{y} - y\right)^T \odot 1 \cdot w_3^T \odot 1 \cdot {\color{red} w_2^T} \odot {\color{green} 1} \\

\end{array}
$$

反向传播计算量主要为矩阵乘法和激活函数：

损失函数求导计算量 $O(2BM_3)$

第三层计算量为 $O(2M_2BM_3 + 3M_2M_3) \approx O(2M_2BM_3)$

第二层计算量为 $O(2M_1BM2 + 2BM_3M_2 + 3M_1M_2) \approx O(2M_1BM_2 + 2BM_3M_2)$

第一层计算量为 $O(2M_0BM_1 + 2BM_2M_1 + 3M_0M_1) \approx O(2M_0BM_1 + 2BM_2M_1)$

在这里，根据公式，微分结果 $\frac{\partial L}{\partial h_i}$ 在某层更新时计算，但是也可以在某层的后一层更新后计算，这样计算量看起来更简洁，即：

损失函数求导计算量 $O(2BM_3)$

第三层计算量为 $O(2M_2BM_3 + 3M_2M_3 + 2BM_3M_2) \approx O(4M_2BM_3)$

第二层计算量为 $O(2M_1BM2 + 3M_1M_2 + 2BM_2M_1) \approx O(4M_1BM_2)$

第一层计算量为 $O(2M_0BM_1 + 3M_0M_1) \approx O(2M_0BM_1)$

可以看出，反向传播计算量主要为矩阵乘法。同样，将MLP模型扩展至 $N+1$ 层，且每个隐藏层的维度为 $M$，最后输出的维度为 $C$，可以得到该MLP模型反向传播的计算量为 $O(2(2N-1)BM^2+6BMC) \approx O(4BNM^2)$

因此**反向传播计算量为前向传播计算量的两倍**。在这里，虽然只推导了MLP模型，但是其他模型（比如CNN、Transformer等）也都是由线性层和激活函数组成，大致关系不会改变，感兴趣的读者可以自行推导。

## 大模型参数量及其训练所需显存之间的关系

我们一般使用单位B（Billion，十亿，$1 \text{B} = 10^9$）表示大模型参数量，比如：Llama-3-8B、Qwen2-72B等，上述二者分别有80亿、720亿个参数，显存单位一般为GB（Gigabyte，吉字节，$1 \text{GB} = 2^{30} \text{Byte} \approx 10^9\text{Byte}$）。

训练一个模型需要的显存主要来自：模型参数、优化器状态、梯度值、激活值和其他。

- 模型参数

假设一个大模型的参数量为 $\mathrm{P}$，深度学习训练常见的浮点数精度为FP32/FP16/BF16（32/16/16比特），即4/2/2字节，不妨假设使用FP16精度，则模型参数占用显存 $2 \mathrm{P}$（字节）。

- 梯度值

反向传播需要存储的梯度值的显存占用和模型参数的一样，为 $2 \mathrm{P}$。

- 优化器状态

假设待优化参数为 $\theta$，目标函数为 $L(\theta)$，学习率为 $\eta$，梯度为 $g_t=\nabla_\theta L(\theta_t)$。

不同的优化器的显存占用不同，大模型训练最常使用Adam作为优化器。这里给出常见优化器的显存占用，以下是常见的优化器及其更新公式：

1. SGD
   $$
   \theta_{t+1} = \theta_{t} - \eta \cdot g_t
   $$

2. Momentum
   $$
   \begin{array}{rl}
   v_{t} &= \gamma v_{t-1} + (1 - \gamma) g_t \\
   \theta_{t+1} &= \theta_{t} - \eta \cdot v_t
   \end{array}
   $$
   其中，$\gamma \in [0,1)$ 为动量系数（通常取0.9），$v_t$ 为累积速度变量。使用梯度的指数移动平均调整动量梯度。

3. RMSProp
   $$
   \begin{array}{rl}
   E[g^2]_{t} &= \rho E[g^2]_{t-1} + (1 - \rho) g_t^2 \\
   \theta_{t+1} &= \theta_{t} - \frac{\eta}{\sqrt{E[g^2]_{t} + \epsilon}} \cdot g_t
   \end{array}
   $$
   其中，$\rho \in [0, 1)$ 为衰减系数（通常取0.9），$\epsilon \approx 10^{-8}$ 为小常数。使用梯度平方的指数移动平均调整每个参数的学习率。

4. Adam
   $$
   \begin{array}{rl}
   m_{t} &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
   v_{t} &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
   \hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
   \hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
   \theta_{t+1} &= \theta_{t} - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}} \cdot \hat{m}_t
   \end{array}
   $$
   其中，$m_t, v_t$ 分别是一阶矩估计（Momentum）和二阶矩估计（RMSProp），$\hat{m}_t, \hat{v}_t$ 分别是偏差修正后的一阶矩估计和二阶矩估计，$\beta_1, \beta_2 \in [0, 1)$ 是矩估计的超参数（通常取0.9和0.999）

> 注：由于指数移动平均在迭代初期存在偏差，如果关心迭代初期的偏差，需要进行偏差修正。其中，Momentum、RMSProp和Adam都使用了指数移动平均，但是只有Adam进行了偏差修正，Momentum和RMSProp通常不进行偏差修正，因为Momentum和RMSProp偏差的影响较小可以忽略，但是Adam结合了二者的方法，放大了偏差的影响，所以进行偏差修正。当然，Momentum和RMSProp也可以进行偏差修正。

不同优化器都需要使用到梯度，这里梯度的显存占用已经在上面被计算了，因此不必重复考虑。

所以下面只考虑不同优化器需要存储的状态，为了避免误差累积和数值稳定以支持参数的稳定高效更新，不管使用任何精度存储模型参数，都使用FP32精度存储优化器状态：

|  优化器  |     状态     |    显存占用    |
| :------: | :----------: | :------------: |
|   SGD    |              |                |
| Momentum |    $v_t$     | $4 \mathrm{P}$ |
| RMSProp  | $E[g^2]_{t}$ | $4 \mathrm{P}$ |
|   Adam   |  $m_t, v_t$  | $8 \mathrm{P}$ |

此外，在优化器更新时，考虑到以下原因，会额外存储一份FP32精度的模型参数的副本，造成 $4 \mathrm{P}$ 的显存占用。

1. 优化器的参数更新是独立于前向传播和反向传播计算图的操作，如果直接在原模型参数上更新，该操作会被记录在计算图中，对计算图造成影响
2. 在混合精度训练时，虽然前向传播和反向传播的计算使用混合精度，但是还是需要使用FP32精度的模型参数副本进行更新，避免数值下溢/溢出，保证更新精度

最后，不妨假设使用Adam优化器，那么优化器状态的显存占用为 $12 \mathrm{P}$。

- 激活值

因为前向传播的激活值可以用于反向传播的计算，所以可以把这个激活值存储下来，避免重复计算，提高效率。

在“前向传播和反向传播之间的关系”中可知，激活值存储的是每一层的输入。在大模型中通常使用Transformer架构，其激活值的显存占用主要和 `batch_size` 和 `sequence_length` 有关。

假设使用经典的仅编码器或者解码器（即，encoder-only或者decoder-only）的Transformer架构，并且**不使用并行训练策略**。

以下是符号约定：

| 符号 |      意义       |
| :--: | :-------------: |
| $v$  |    词表大小     |
| $b$  |   微批量大小    |
| $L$  | Transformer层数 |
| $s$  |    序列长度     |
| $h$  |   隐藏层维度    |
| $a$  | 注意力头的个数  |

以下是Transformer的组成：

> 说明：
>
> Transformer由嵌入层（Embedding）、$L$ 个Transformer层、层归一化（Layer Norm）和输出层（Output Layer）组成；
>
> Embedding由词嵌入（Word Embedding）、位置嵌入（Position Embedding）和Dropout组成；
>
> 每个Transformer层按顺序由层归一化（Layer Norm）、注意力层（Attention）、层归一化（Layer Norm）和多层感知机（MLP），其余同理。

|         Transformer          |                    |                |    数据形状     |
| :--------------------------: | :----------------: | :------------: | :-------------: |
|          Embedding           |                    |                |  $bs \to bsh$   |
|                              |   Word Embedding   |                |  $bs \to bsh$   |
|                              | Position Embedding |                |  $bsh \to bsh$  |
|                              |      Dropout       |                |  $bsh \to bsh$  |
| $L \times$ Transformer Layer |                    |                |  $bsh \to bsh$  |
|                              |     Layer Norm     |                |  $bsh \to bsh$  |
|                              |     Attention      |                |  $bsh \to bsh$  |
|                              |                    | Self Attention |  $bsh \to bsh$  |
|                              |                    |     Linear     |  $bsh \to bsh$  |
|                              |                    |    Dropout     |  $bsh \to bsh$  |
|                              |     Layer Norm     |                |  $bsh \to bsh$  |
|                              |        MLP         |                |  $bsh \to bsh$  |
|                              |                    |     Linear     | $bsh \to bs4h$  |
|                              |                    |      GeLU      | $bs4h \to bs4h$ |
|                              |                    |     Linear     | $bs4h \to bsh$  |
|                              |                    |    Dropout     |  $bsh \to bsh$  |
|          Layer Norm          |                    |                |  $bsh \to bsh$  |
|         Output Layer         |                    |                |  $bsh \to bsh$  |

其中，自注意力的公式和激活值及其显存占用如下：

|                    Self Attention                     |         数据形状          |    激活值     |    显存占用     |  说明   |
| :---------------------------------------------------: | :-----------------------: | :-----------: | :-------------: | :-----: |
|           $Q = X W_Q, K = X W_K, V = X W_V$           |  $bsh \times hh \to bsh$  |      $X$      |     $2bsh$      |         |
|                      $S = QK^T$                       | $bsh \times bsh \to bs^2$ |     $Q,K$     |     $4bsh$      |         |
| $S = \text{softmax}\left( \frac{S}{\sqrt{h}} \right)$ |      $bs^2 \to bs^2$      |      $S$      |    $2bs^2a$     |         |
|               $S = S \odot \text{mask}$               |      $bs^2 \to bs^2$      | $\text{mask}$ |     $bs^2a$     | Dropout |
|                       $S = S V$                       | $bs^2 \times bsh \to bsh$ |    $S, V$     | $2bs^2a + 2bsh$ |         |

所以Self Attention的显存占用为 $8bsh+5bs^2a$。

那么每个Transformer层激活值的显存占用如下：

| Transformer Layer |                |    数据形状     |     激活值      |         显存占用         |    说明    |
| :---------------: | :------------: | :-------------: | :-------------: | :----------------------: | :--------: |
|    Layer Norm     |                |  $bsh \to bsh$  | $\mu, \sigma^2$ | $sbh + 2sb \approx sbh $ | 均值、方差 |
|     Attention     |                |  $bsh \to bsh$  |     $\dots$     |      $11bsh+5bs^2a$      |            |
|                   | Self Attention |  $bsh \to bsh$  |     $\dots$     |      $8bsh+5bs^2a$       |            |
|                   |     Linear     |  $bsh \to bsh$  |       $X$       |          $2bsh$          |    输入    |
|                   |    Dropout     |  $bsh \to bsh$  |  $\text{mask}$  |          $bsh$           |            |
|    Layer Norm     |                |  $bsh \to bsh$  | $\mu, \sigma^2$ | $sbh + 2sb \approx sbh $ | 均值、方差 |
|        MLP        |                |  $bsh \to bsh$  |     $\dots$     |         $19bsh$          |            |
|                   |     Linear     | $bsh \to bs4h$  |       $X$       |          $2bsh$          |    输入    |
|                   |      GeLU      | $bs4h \to bs4h$ |       $X$       |          $8bsh$          |    输入    |
|                   |     Linear     | $bs4h \to bsh$  |       $X$       |          $8bsh$          |    输入    |
|                   |    Dropout     |  $bsh \to bsh$  |  $\text{mask}$  |          $bsh$           |            |

所以每个Transformer层的显存占用为 $34bsh+5bs^2a$。

因此可以得到Transformer的显存占用为：

|         Transformer          |   数据形状    | 激活值  |                 显存占用                 | 说明 |
| :--------------------------: | :-----------: | :-----: | :--------------------------------------: | :--: |
|          Embedding           | $bs \to bsh$  |   $X$   |                  $bsh$                   | 输入 |
| $L \times$ Transformer Layer | $bsh \to bsh$ | $\dots$ | $bshL \left( 34 + 5\frac{as}{h} \right)$ |      |
|          Layer Norm          | $bsh \to bsh$ |   $X$   |         $sbh + 2sb \approx sbh $         | 输入 |
|         Output Layer         | $bsh \to bsh$ |   $X$   |                  $2bsh$                  | 输入 |

可以看出主要的显存占用在Transformer层。

此外，如果使用了并行训练策略，比如管道并行、张量并行，那么每张卡上面的显存占用也会有所变化，感兴趣的读者可以自行阅读原论文。

最后，为了方便计算，非常粗略地估计（也可以说是，不要脸地直接假设）激活值的显存占用为 $2 \mathrm{P}$。

- 其他

如果使用了一定的训练技巧，显存占用会有所变化，比如：重计算、LoRA等。

以上，一个参数量为 $\mathrm{P}$ 的大模型，如果使用FP16精度和Adam优化器，其显存占用大约为 $2 \mathrm{P} + 2 \mathrm{P} + 12 \mathrm{P} + 2 \mathrm{P} = 18 \mathrm{P}$；如果使用FP32精度和Adam优化器，其显存占用大约为 $4 \mathrm{P} + 4 \mathrm{P} + 12 \mathrm{P} + 4 \mathrm{P} = 24 \mathrm{P}$。

## 大模型缩放定律推导

大模型缩放定律指的是模型性能随着模型参数数量、训练数据量以及计算资源的增加而呈现出可预测的增长规律。



deepseek v3中扩展的缩放定律







参考：

[矩阵求偏导-CSDN博客](https://blog.csdn.net/lirika_777/article/details/79646453)

[【ML-0-3】矩阵求导-链式法则 - 忆凡人生 - 博客园](https://www.cnblogs.com/yifanrensheng/p/12639539.html)

[矩阵求导知识点总结 - GYHHAHA - 博客园](https://www.cnblogs.com/gyhhaha/p/11782212.html)

[为什么反向计算是前向耗时的两倍？ - 知乎](https://zhuanlan.zhihu.com/p/717782439)

[反向传播，计算量为什么是前向两倍？ - 知乎](https://zhuanlan.zhihu.com/p/30558763820)

[浅谈后向传递的计算量大约是前向传递的两倍 - 知乎](https://zhuanlan.zhihu.com/p/675517271)



[2205.05198](https://arxiv.org/pdf/2205.05198)

[[2001.08361\] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)



[[LLM\]大模型显存计算公式与优化 - 知乎](https://zhuanlan.zhihu.com/p/687226668)



[为什么混合精度训练中优化器参数仍然以 FP32 存储？LLaMA 2 7B 模型在混合精度下的显存需求-EW帮帮网](https://www.ewbang.com/community/article/details/1000108736.html)
