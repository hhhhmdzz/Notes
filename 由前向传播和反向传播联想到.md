# 由前向传播和反向传播联想到

## 矩阵乘法的时间复杂度和计算量

假设有两个矩阵进行矩阵乘法 $C=AB$，矩阵 $A,B,C$ 的形状分别为 $(N, M), (M, P), (N, P)$。矩阵 $C$ 有 $NP$ 个元素，每个元素需要进行 $M$ 次乘法和 $M-1$ 次加法得到，所以矩阵乘法的**时间复杂度**为 $O(NMP)$，**计算量**为 $O\left( \left(2 M - 1 \right) N P \right) \approx O(2NMP)$。多个矩阵进行矩阵乘法的时间复杂度和计算量同理，略。

## 矩阵形式的链式法则

向量 $\vec{x} = \left(x_1, x_2, \dots, x_n \right), \vec{u} = \left(u_1, u_2, \dots, u_k \right), \vec{y} = \left(y_1, y_2, \dots, y_m \right)$

矩阵 $\bold{x}, \bold{u}, \bold{y}$

向量和矩阵求导规则：

1. $y = f(x), \frac{d y}{d x} = f^{'}$
2. $y = f(\vec{x}), \frac{\partial y}{\partial \vec{x}} = \left(\frac{\partial  y}{\partial x_1}, \frac{\partial  y}{\partial x_2}, \dots, \frac{\partial  y}{\partial x_n}\right)$
3. $\vec{y} = f(\vec{x}), \frac{\partial \vec{y}}{\partial \vec{x}} = \left[\begin{matrix} \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \dots & \frac{\partial y_1}{\partial x_n} \\ \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \dots & \frac{\partial y_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \dots & \frac{\partial y_m}{\partial x_n} \\ \end{matrix} \right]$（雅可比矩阵）

假设存在 $\bold{\hat{Y}} = \bold{A} \bold{X} + \bold{B}, L = F(\bold{\hat{Y}}, \bold{Y}) = \sum_{i_p=1}^{P} \sum_{i_m=1}^{M} f(\hat{y}_{i_m,i_p}, y_{i_m,i_p})$，矩阵 $\bold{\hat{Y}}, \bold{A}, \bold{X}, \bold{B}$ 的形状分别为 $(M, P), (M, N), (N, P), (M, P)$，矩阵元素使用小写字母表示为 $\hat{y}, a, x, b$。

其中 $\hat{y}_{i_m,i_p} = \sum_{i_n=1}^{N} a_{i_m,i_n} x_{i_n,i_p}$，则有 $\frac{\partial \hat{y}_{i_m,i_p}}{\partial x_{n,i_p}} = a_{i_m,n}$

求 $L$ 关于 $x_{n,p}$ 的偏导：
$$
\begin{array}{l l}
\frac{\partial L}{\partial x_{n,p}} &= \frac{\partial }{\partial x_{n,p}} \sum_{i_p=1}^{P} \sum_{i_m=1}^{M} f(\hat{y}_{i_m,i_p}, y_{i_m,i_p}) \\
&= \frac{\partial }{\partial x_{n,p}} \sum_{i_m=1}^{M} f(\hat{y}_{i_m,p}, y_{i_m,p}) \\
&= \sum_{i_m=1}^{M} \frac{\partial }{\partial \hat{y}_{i_m,p} } \frac{\partial \hat{y}_{i_m,p} }{\partial x_{n,p}} f(\hat{y}_{i_m,p}, y_{i_m,p}) \\
&= \sum_{i_m=1}^{M} \frac{\partial f(\hat{y}_{i_m,p}, y_{i_m,p})}{\partial \hat{y}_{i_m,p} } \frac{\partial \hat{y}_{i_m,p} }{\partial x_{n,p}} \\
&= \sum_{i_m=1}^{M} a_{i_m,n} \frac{\partial f(\hat{y}_{i_m,p}, y_{i_m,p})}{\partial \hat{y}_{i_m,p} } \\
&= \bold{A}_{:,n}^T \frac{\partial L}{\partial \bold{\hat{Y}}_{:,p}} \\
\end{array}
$$
所以矩阵形式的**链式法则**为：
$$
\frac{\partial{L}}{\partial{\bold{X}}} = \bold{A}^T \frac{\partial{L}}{\partial{\bold{\hat{Y}}}}
$$

## 前向传播和反向传播之间的关系

假设有以下MLP模型，假设激活函数 $\text{act}(x) = x$，损失函数为均方误差损失。

不妨假设 $w_1, w_2, w_3$ 的维度分别为 $(M_0, M_1), (M_1, M_2), (M_2, M_3)$

那么 $x, z_1, z_2, \hat{y}$ 的维度分别为 $(B, M_0), (B, M_1), (B, M_2), (B, M_3)$

- 前向传播

前向传播过程为：
$$
\begin{array}{r l}
z_1 &= \text{act}(h_1), h_1 = x w_1 \\
z_2 &= \text{act}(h_2), h_2 = z_1 w_2 \\
\hat{y} &= \text{act}(h_3), h_3 = z_2 w_3 \\
L &= L(x, y, w_1, w_2, w_3) = \Vert\hat{y} - y\Vert_2^2 \\
\end{array}
$$

前向传播计算量主要为矩阵乘法和激活函数：

第一层计算量为 $O(2BM_0M_1 + BM_1) \approx O(2BM_0M_1)$

第二层计算量为 $O(2BM_1M_2 + BM_2) \approx O(2BM_1M_2)$

第三层计算量为 $O(2BM_2M_3 + BM_3) \approx O(2BM_2M_3)$

损失函数计算量为 $O(2BM_3)$

可以看出，前向传播计算量主要为矩阵乘法。将MLP模型扩展至 $N+1$ 层，且每个隐藏层的维度为 $M$，最后输出的维度为 $C$，可以得到该MLP模型前向传播的计算量为 $O(2NBM^2+2BMC) \approx O(2BNM^2)$。

- 反向传播

反向传播过程为：
$$
\begin{array}{l l}

w_3 &:= w_3 - \alpha \cdot {\color{red} \frac{\partial L}{\partial \hat{y}}} \cdot {\color{green} \frac{\partial \hat{y}}{\partial h_3}} \cdot {\color{blue} \frac{\partial h_3}{\partial w_3}}
& = w_3 - \alpha \cdot {\color{blue} z_2^T} \cdot  {\color{red} 2 \left(\hat{y} - y\right)} \odot {\color{green} 1} \\

w_2 &:= w_2 - \alpha \cdot \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial h_3} \cdot {\color{red} \frac{\partial h_3}{\partial z_2}} \cdot {\color{green} \frac{\partial z_2}{\partial h_2}} \cdot {\color{blue} \frac{\partial h_2}{\partial w_2}}
& = w_2 - \alpha \cdot {\color{blue} z_1^T} \cdot 2 \left(\hat{y} - y\right)^T \odot 1 \cdot {\color{red} w_3^T}\odot {\color{green} 1}  \\

w_1 &:= w_1 - \alpha \cdot \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial h_3} \cdot \frac{\partial h_3}{\partial z_2} \cdot \frac{\partial z_2}{\partial h_2} \cdot {\color{red} \frac{\partial h_2}{\partial z_1}} \cdot {\color{green} \frac{\partial z_1}{\partial h_1}} \cdot {\color{blue} \frac{\partial h_1}{\partial w_1}}
& = w_1 - \alpha  \cdot {\color{blue} x^T} \cdot 2 \left(\hat{y} - y\right)^T \odot 1 \cdot w_3^T \odot 1 \cdot {\color{red} w_2^T} \odot {\color{green} 1} \\

\end{array}
$$

反向传播计算量主要为矩阵乘法和激活函数：

> 注：激活函数的矩阵乘法 $\cdot$ 可以转换为矩阵逐元素乘法 $\odot$

损失函数求导计算量 $O(2BM_3)$

第三层计算量为 $O(2M_2BM_3 + 3M_2M_3) \approx O(2M_2BM_3)$

第二层计算量为 $O(2M_1BM2 + 2BM_3M_2 + 3M_1M_2) \approx O(2M_1BM_2 + 2BM_3M_2)$

第一层计算量为 $O(2M_0BM_1 + 2BM_2M_1 + 3M_0M_1) \approx O(2M_0BM_1 + 2BM_2M_1)$

在这里，根据公式，微分结果 $\frac{\partial L}{\partial h_i}$ 在某层更新时计算，但是也可以在某层的后一层更新后计算，这样计算量看起来更简洁，即：

损失函数求导计算量 $O(2BM_3)$

第三层计算量为 $O(2M_2BM_3 + 3M_2M_3 + 2BM_3M_2) \approx O(4M_2BM_3)$

第二层计算量为 $O(2M_1BM2 + 3M_1M_2 + 2BM_2M_1) \approx O(4M_1BM_2)$

第一层计算量为 $O(2M_0BM_1 + 3M_0M_1) \approx O(2M_0BM_1)$

可以看出，反向传播计算量主要为矩阵乘法。同样，将MLP模型扩展至 $N+1$ 层，且每个隐藏层的维度为 $M$，最后输出的维度为 $C$，可以得到该MLP模型反向传播的计算量为 $O(2(2N-1)BM^2+6BMC) \approx O(4BNM^2)$

因此**反向传播计算量为前向传播计算量的两倍**。在这里，虽然只推导了MLP模型，但是其他模型（比如CNN、Transformer等）也都是由线性层和激活函数组成，大致关系不会改变，感兴趣的读者可以自行推导。

## 训练大模型所需显存和模型大小之间的关系







## 大模型缩放定律推导







参考：

[矩阵求偏导-CSDN博客](https://blog.csdn.net/lirika_777/article/details/79646453)

[【ML-0-3】矩阵求导-链式法则 - 忆凡人生 - 博客园](https://www.cnblogs.com/yifanrensheng/p/12639539.html)

[矩阵求导知识点总结 - GYHHAHA - 博客园](https://www.cnblogs.com/gyhhaha/p/11782212.html)

[为什么反向计算是前向耗时的两倍？ - 知乎](https://zhuanlan.zhihu.com/p/717782439)

[反向传播，计算量为什么是前向两倍？ - 知乎](https://zhuanlan.zhihu.com/p/30558763820)

[浅谈后向传递的计算量大约是前向传递的两倍 - 知乎](https://zhuanlan.zhihu.com/p/675517271)





